import pandas as pd
import numpy as np
import requests
import json
import os
import time
from datetime import datetime
import warnings
import random
import logging
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                    handlers=[logging.FileHandler("stock_data_akshare.log"), logging.StreamHandler()])
logger = logging.getLogger('stock_data_akshare_fetcher')

# æ£€æŸ¥akshareå¯ç”¨æ€§
try:
    import akshare as ak
    AKSHARE_AVAILABLE = True
except ImportError:
    AKSHARE_AVAILABLE = False
    logger.warning("âš ï¸  akshareåº“æœªå®‰è£…ï¼Œè¿è¡Œ `pip install akshare` ä»¥å¯ç”¨akshareæ•°æ®æº")

# ç¡®ä¿cacheç›®å½•å­˜åœ¨
if not os.path.exists('cache'):
    os.makedirs('cache')

# åçˆ¬é…ç½®
class AntiCrawlConfig:
    # éšæœºUser-Agentåˆ—è¡¨
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:94.0) Gecko/20100101 Firefox/94.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36 Edg/96.0.1054.29"
    ]
    
    # è¯·æ±‚å»¶è¿Ÿé…ç½® (ç§’)
    MIN_DELAY = 0.5  # æœ€å°å»¶è¿Ÿ
    MAX_DELAY = 2.0  # æœ€å¤§å»¶è¿Ÿ
    
    # æ‰¹æ¬¡å¤„ç†å»¶è¿Ÿé…ç½® (ç§’)
    BATCH_MIN_DELAY = 3.0  # æ‰¹æ¬¡é—´æœ€å°å»¶è¿Ÿ
    BATCH_MAX_DELAY = 5.0  # æ‰¹æ¬¡é—´æœ€å¤§å»¶è¿Ÿ
    
    # å•åªè‚¡ç¥¨å¤„ç†é—´éš” (ç§’)
    STOCK_MIN_INTERVAL = 0.5  # è‚¡ç¥¨é—´æœ€å°é—´éš”
    
    # é‡è¯•é…ç½®
    MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
    RETRY_BACKOFF_FACTOR = 0.3  # é‡è¯•é€€é¿å› å­

# åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„session
def create_session():
    session = requests.Session()
    retry = Retry(total=AntiCrawlConfig.MAX_RETRIES,
                 backoff_factor=AntiCrawlConfig.RETRY_BACKOFF_FACTOR,
                 status_forcelist=[500, 502, 503, 504, 429])
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

# æ·»åŠ éšæœºå»¶è¿Ÿ
def add_random_delay(min_delay=None, max_delay=None):
    min_delay = min_delay or AntiCrawlConfig.MIN_DELAY
    max_delay = max_delay or AntiCrawlConfig.MAX_DELAY
    delay = random.uniform(min_delay, max_delay)
    time.sleep(delay)

# æ•°æ®è®¿é—®æ§åˆ¶ç±»
class RateLimiter:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(RateLimiter, cls).__new__(cls)
            cls._instance.reset()
        return cls._instance
    
    def reset(self):
        self.request_counts = {
            'akshare': 0,
            'eastmoney': 0
        }
        self.last_reset_times = {
            'akshare': time.time(),
            'eastmoney': time.time()
        }
        self.rate_limits = {
            'akshare': 60,  # æ¯åˆ†é’Ÿæœ€å¤šè¯·æ±‚æ•°
            'eastmoney': 50
        }
    
    def check_rate_limit(self, source):
        current_time = time.time()
        elapsed = current_time - self.last_reset_times[source]
        
        # æ¯åˆ†é’Ÿé‡ç½®è®¡æ•°
        if elapsed >= 60:
            self.request_counts[source] = 0
            self.last_reset_times[source] = current_time
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
        if self.request_counts[source] >= self.rate_limits[source]:
            wait_time = 60 - elapsed + 1  # ç­‰å¾…åˆ°ä¸‹ä¸€åˆ†é’Ÿå†ç»§ç»­
            logger.warning(f"[{source}] å·²è¾¾è¯·æ±‚é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’")
            time.sleep(wait_time)
            self.request_counts[source] = 0
            self.last_reset_times[source] = time.time()
        
        # å¢åŠ è®¡æ•°
        self.request_counts[source] += 1

# åˆå§‹åŒ–è®¿é—®æ§åˆ¶å™¨
rate_limiter = RateLimiter()

warnings.filterwarnings('ignore')

def get_stock_list():
    """ä»æœ¬åœ°æ–‡ä»¶è¯»å–è‚¡ç¥¨åˆ—è¡¨"""
    try:
        stock_list = pd.read_csv('cache/stockA_list.csv')
        print(f"âœ… æˆåŠŸè¯»å–è‚¡ç¥¨åˆ—è¡¨ï¼Œå…±{len(stock_list)}åªè‚¡ç¥¨")
        return stock_list
    except Exception as e:
        print(f"âŒ è¯»å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
        return None

def load_progress():
    """åŠ è½½è¿›åº¦ä¿¡æ¯"""
    progress_file = 'cache/fundamentals_akshare_progress.json'
    if os.path.exists(progress_file):
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {"last_index": 0, "completed_codes": []}

def save_progress(index, completed_codes):
    """ä¿å­˜è¿›åº¦ä¿¡æ¯"""
    progress_file = 'cache/fundamentals_akshare_progress.json'
    try:
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump({"last_index": index, "completed_codes": completed_codes}, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜è¿›åº¦å¤±è´¥: {e}")

def get_fundamentals_from_akshare(code):
    """ä½¿ç”¨akshareè·å–è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®ï¼ŒåŒ…å«è®¿é—®æ§åˆ¶ç­–ç•¥"""
    if not AKSHARE_AVAILABLE:
        return None
    
    try:
        import akshare as ak  # ç¡®ä¿akå˜é‡åœ¨å‡½æ•°å†…éƒ¨å¯ç”¨
        
        # æ£€æŸ¥è®¿é—®é¢‘ç‡é™åˆ¶
        rate_limiter.check_rate_limit('akshare')
        
        # è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
        try:
            stock_info = ak.stock_individual_info_em(symbol=code)
            stock_name = str(stock_info.loc[stock_info['item'] == 'è‚¡ç¥¨ç®€ç§°', 'value'].iloc[0]) if not stock_info.empty else ''
            ipo_date = str(stock_info.loc[stock_info['item'] == 'ä¸Šå¸‚æ—¶é—´', 'value'].iloc[0]) if not stock_info.empty else ''
            industry = str(stock_info.loc[stock_info['item'] == 'è¡Œä¸š', 'value'].iloc[0]) if not stock_info.empty else ''
            
            # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé˜²æ­¢è¯·æ±‚è¿‡å¿«
            add_random_delay()
        except Exception as e:
            logger.warning(f"è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            stock_name = ''
            ipo_date = ''
            industry = ''
        
        # è·å–è´¢åŠ¡æŒ‡æ ‡æ•°æ® - ä½¿ç”¨å¤šä¸ªå¤‡ç”¨æ¥å£
        latest_data = {}
        profit_data = {}
        valuation_data = {}
        
        # æ–¹æ³•1: ä½¿ç”¨è´¢åŠ¡æ‘˜è¦
        try:
            financial_indicator = ak.stock_financial_abstract(symbol=code)
            if not financial_indicator.empty:
                latest_data = financial_indicator.iloc[0].to_dict() if hasattr(financial_indicator.iloc[0], 'to_dict') else {}
            
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            add_random_delay()
        except Exception as e:
            logger.warning(f"è·å–è´¢åŠ¡æ‘˜è¦å¤±è´¥: {e}")
            pass
        
        # æ–¹æ³•2: ä½¿ç”¨è´¢åŠ¡åˆ†ææŒ‡æ ‡
        try:
            profit_ability = ak.stock_financial_analysis_indicator(symbol=code)
            if not profit_ability.empty:
                profit_data = profit_ability.iloc[0].to_dict() if hasattr(profit_ability.iloc[0], 'to_dict') else {}
            
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            add_random_delay()
        except Exception as e:
            logger.warning(f"è·å–è´¢åŠ¡åˆ†ææŒ‡æ ‡å¤±è´¥: {e}")
            pass
        
        # æ–¹æ³•3: ä½¿ç”¨ä¸ªè‚¡ä¼°å€¼æŒ‡æ ‡
        try:
            # å…ˆæ·»åŠ è¾ƒé•¿å»¶è¿Ÿ
            time.sleep(1.0)
            
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ¥å£è·å–ä¼°å€¼æ•°æ®
            try:
                # æ–¹æ³•A: ä½¿ç”¨å®æ—¶è¡Œæƒ…æ¥å£ï¼ˆakshareï¼‰
                stock_zh_a_spot = ak.stock_zh_a_spot()
                stock_data = stock_zh_a_spot[stock_zh_a_spot['ä»£ç '] == code]
                if not stock_data.empty:
                    valuation_data = {
                        'å¸‚ç›ˆç‡': str(stock_data.iloc[0].get('å¸‚ç›ˆç‡', '')),
                        'å¸‚å‡€ç‡': str(stock_data.iloc[0].get('å¸‚å‡€ç‡', '')),
                        'è‚¡æ¯ç‡': str(stock_data.iloc[0].get('è‚¡æ¯ç‡', ''))
                    }
            except Exception as inner_e:
                logger.warning(f"Aè‚¡å®æ—¶è¡Œæƒ…æ¥å£å¤±è´¥: {inner_e}")
                # æ–¹æ³•B: é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ä¸œæ–¹è´¢å¯Œçš„ä¸ªè‚¡è¡Œæƒ…æ¥å£
                try:
                    # å…ˆæ·»åŠ è¾ƒé•¿å»¶è¿Ÿ
                    time.sleep(1.5)
                    # ç›´æ¥ä½¿ç”¨requestsè®¿é—®ä¸œæ–¹è´¢å¯ŒAPIï¼Œæ·»åŠ å®Œæ•´çš„åçˆ¬è¯·æ±‚å¤´
                    session = create_session()
                    headers = {
                        'User-Agent': random.choice(AntiCrawlConfig.USER_AGENTS),
                        'Accept': 'application/json, text/javascript, */*; q=0.01',
                        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                        'Referer': f'https://quote.eastmoney.com/{code}.html',
                        'X-Requested-With': 'XMLHttpRequest',
                        'Connection': 'keep-alive',
                        'Cache-Control': 'no-cache'
                    }
                    secid = f"0.{code}" if str(code).startswith(('0', '3')) else f"1.{code}"
                    url = f"http://push2.eastmoney.com/api/qt/stock/get?secid={secid}&fields=f163,f164,f167,f168,f188"
                    response = session.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        try:
                            data = response.json()
                            if 'data' in data:
                                stock_data = data['data']
                                valuation_data = {
                                    'å¸‚ç›ˆç‡': str(stock_data.get('f164', '')),  # å¸‚ç›ˆç‡(TTM)
                                    'å¸‚å‡€ç‡': str(stock_data.get('f167', '')),
                                    'è‚¡æ¯ç‡': str(stock_data.get('f188', ''))
                                }
                        except json.JSONDecodeError as json_e:
                            logger.warning(f"ä¸œæ–¹è´¢å¯ŒAPIè¿”å›æ ¼å¼é”™è¯¯ï¼Œå¯èƒ½æ˜¯HTML: {json_e}")
                except Exception as inner_e3:
                    logger.warning(f"ä¸œæ–¹è´¢å¯Œä¼°å€¼æ¥å£å¤±è´¥: {inner_e3}")
            
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            add_random_delay(1.0, 2.0)
        except Exception as e:
            logger.warning(f"è·å–ä¸ªè‚¡ä¼°å€¼æŒ‡æ ‡å¤±è´¥: {e}")
            pass
        
        # æ–¹æ³•4: ä½¿ç”¨ä¸œè´¢çš„è´¢åŠ¡æ•°æ®æ¥å£
        try:
            stock_financial = ak.stock_financial_analysis_indicator(symbol=code)
            if not stock_financial.empty:
                financial_dict = stock_financial.iloc[0].to_dict() if hasattr(stock_financial.iloc[0], 'to_dict') else {}
                # åˆå¹¶æ•°æ®
                for key, value in financial_dict.items():
                    if key not in latest_data:
                        latest_data[key] = value
        except Exception as e:
            logger.warning(f"è·å–ä¸œè´¢è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
            pass
        
        # æ„å»ºå®Œæ•´çš„åŸºæœ¬é¢æ•°æ®
        fundamental = {
            'è‚¡ç¥¨ä»£ç ': code,
            'è‚¡ç¥¨åç§°': stock_name,
            'è‚¡ç¥¨ä¸Šå¸‚æ—¥æœŸ': ipo_date,
            'è‚¡ç¥¨ä¸Šå¸‚åœ°ç‚¹': 'ä¸Šæµ·' if str(code).startswith(('6', '5')) else 'æ·±åœ³',
            'è‚¡ç¥¨æ‰€å±è¡Œä¸š': industry,
            
            # ç›ˆåˆ©èƒ½åŠ›
            'æ¯è‚¡æ”¶ç›Š': str(latest_data.get('åŸºæœ¬æ¯è‚¡æ”¶ç›Š', '') or latest_data.get('æ¯è‚¡æ”¶ç›Š', '')),
            'æ¯è‚¡å‡€èµ„äº§': str(latest_data.get('æ¯è‚¡å‡€èµ„äº§', '') or latest_data.get('å‡€èµ„äº§', '')),
            'å‡€èµ„äº§æ”¶ç›Šç‡': str(profit_data.get('å‡€èµ„äº§æ”¶ç›Šç‡', '') or latest_data.get('å‡€èµ„äº§æ”¶ç›Šç‡', '')),
            'æ€»èµ„äº§æ”¶ç›Šç‡': str(profit_data.get('æ€»èµ„äº§æŠ¥é…¬ç‡', '') or latest_data.get('æ€»èµ„äº§æ”¶ç›Šç‡', '')),
            'æ¯›åˆ©ç‡': str(profit_data.get('é”€å”®æ¯›åˆ©ç‡', '') or latest_data.get('æ¯›åˆ©ç‡', '')),
            'å‡€åˆ©ç‡': str(profit_data.get('é”€å”®å‡€åˆ©ç‡', '') or latest_data.get('å‡€åˆ©ç‡', '')),
            'è¥ä¸šåˆ©æ¶¦ç‡': str(profit_data.get('è¥ä¸šåˆ©æ¶¦ç‡', '') or latest_data.get('è¥ä¸šåˆ©æ¶¦ç‡', '')),
            
            # ä¼°å€¼æŒ‡æ ‡
            'å¸‚ç›ˆç‡ï¼ˆé™ï¼‰': str(valuation_data.get('å¸‚ç›ˆç‡', '')),
            'å¸‚ç›ˆç‡ï¼ˆTTMï¼‰': str(valuation_data.get('å¸‚ç›ˆç‡TTM', '')),
            'å¸‚å‡€ç‡': str(valuation_data.get('å¸‚å‡€ç‡', '')),
            'å¸‚é”€ç‡': str(valuation_data.get('å¸‚é”€ç‡', '')),
            'è‚¡æ¯ç‡': str(valuation_data.get('è‚¡æ¯ç‡', '')),
            
            # æˆé•¿æ€§
            'è¥ä¸šæ”¶å…¥å¢é•¿ç‡': str(latest_data.get('è¥ä¸šæ”¶å…¥å¢é•¿ç‡', '') or latest_data.get('è¥æ”¶å¢é•¿ç‡', '')),
            'å‡€åˆ©æ¶¦å¢é•¿ç‡': str(latest_data.get('å‡€åˆ©æ¶¦å¢é•¿ç‡', '') or latest_data.get('å‡€åˆ©æ¶¦å¢é€Ÿ', '')),
            'å‡€èµ„äº§å¢é•¿ç‡': str(latest_data.get('å‡€èµ„äº§å¢é•¿ç‡', '')),
            'å‡€åˆ©æ¶¦å¢é€Ÿ': str(latest_data.get('å‡€åˆ©æ¶¦å¢é•¿ç‡', '') or latest_data.get('å‡€åˆ©æ¶¦å¢é€Ÿ', '')),
            
            # å¿å€ºèƒ½åŠ›
            'èµ„äº§è´Ÿå€ºç‡': str(latest_data.get('èµ„äº§è´Ÿå€ºç‡', '')),
            'æµåŠ¨æ¯”ç‡': str(latest_data.get('æµåŠ¨æ¯”ç‡', '')),
            'é€ŸåŠ¨æ¯”ç‡': str(latest_data.get('é€ŸåŠ¨æ¯”ç‡', '')),
            
            # è¿è¥èƒ½åŠ›
            'æ€»èµ„äº§å‘¨è½¬ç‡': str(latest_data.get('æ€»èµ„äº§å‘¨è½¬ç‡', '')),
            'å­˜è´§å‘¨è½¬ç‡': str(latest_data.get('å­˜è´§å‘¨è½¬ç‡', '')),
            'åº”æ”¶è´¦æ¬¾å‘¨è½¬ç‡': str(latest_data.get('åº”æ”¶è´¦æ¬¾å‘¨è½¬ç‡', '')),
            
            # ç°é‡‘æµ
            'æ¯è‚¡ç»è¥ç°é‡‘æµ': str(latest_data.get('æ¯è‚¡ç»è¥ç°é‡‘æµ', '') or latest_data.get('ç»è¥ç°é‡‘æµ', '')),
            'ç°é‡‘æµé‡æ¯”ç‡': str(latest_data.get('ç°é‡‘æµé‡æ¯”ç‡', ''))
        }
        
        return fundamental
        
    except Exception as e:
        logger.error(f"âŒ akshareè·å– {code} æ•°æ®å¤±è´¥: {str(e)}")
        return None

def get_fundamentals_real_data(code):
    """è·å–å•åªè‚¡ç¥¨çš„çœŸå®åŸºæœ¬é¢æ•°æ®"""
    
    # ä½¿ç”¨akshareè·å–æ•°æ®
    result = get_fundamentals_from_akshare(code)
    if result and result.get('è‚¡ç¥¨åç§°') and result.get('è‚¡ç¥¨åç§°') != '':
        logger.info(f"   âœ… ä½¿ç”¨akshareè·å– {code} æ•°æ®æˆåŠŸ")
        return result
    
    # å¦‚æœæ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
    logger.warning(f"   âŒ è·å– {code} æ•°æ®å¤±è´¥")
    return {
        'è‚¡ç¥¨ä»£ç ': code,
        'è‚¡ç¥¨åç§°': '',
        'è‚¡ç¥¨ä¸Šå¸‚æ—¥æœŸ': '',
        'è‚¡ç¥¨ä¸Šå¸‚åœ°ç‚¹': 'ä¸Šæµ·' if str(code).startswith(('6', '5')) else 'æ·±åœ³',
        'è‚¡ç¥¨æ‰€å±è¡Œä¸š': ''
    }

def save_batch_to_csv(batch_data, mode='a'):
    """å°†æ‰¹æ¬¡æ•°æ®ä¿å­˜åˆ°CSVæ–‡ä»¶"""
    try:
        df = pd.DataFrame(batch_data)
        
        # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
        numeric_columns = [
            'æ¯è‚¡æ”¶ç›Š', 'æ¯è‚¡å‡€èµ„äº§', 'å‡€èµ„äº§æ”¶ç›Šç‡', 'æ€»èµ„äº§æ”¶ç›Šç‡', 'æ¯›åˆ©ç‡', 'å‡€åˆ©ç‡', 'è¥ä¸šåˆ©æ¶¦ç‡',
            'å¸‚ç›ˆç‡ï¼ˆé™ï¼‰', 'å¸‚ç›ˆç‡ï¼ˆTTMï¼‰', 'å¸‚å‡€ç‡', 'å¸‚é”€ç‡', 'è‚¡æ¯ç‡',
            'è¥ä¸šæ”¶å…¥å¢é•¿ç‡', 'å‡€åˆ©æ¶¦å¢é•¿ç‡', 'å‡€èµ„äº§å¢é•¿ç‡', 'å‡€åˆ©æ¶¦å¢é€Ÿ',
            'èµ„äº§è´Ÿå€ºç‡', 'æµåŠ¨æ¯”ç‡', 'é€ŸåŠ¨æ¯”ç‡',
            'æ€»èµ„äº§å‘¨è½¬ç‡', 'å­˜è´§å‘¨è½¬ç‡', 'åº”æ”¶è´¦æ¬¾å‘¨è½¬ç‡',
            'æ¯è‚¡ç»è¥ç°é‡‘æµ', 'ç°é‡‘æµé‡æ¯”ç‡'
        ]
        
        for col in df.columns:
            if col in numeric_columns:
                df[col] = df[col].replace(['', 'None', 'nan'], np.nan)
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        csv_path = 'cache/stockA_fundamentals_akshare.csv'
        if mode == 'w' or not os.path.exists(csv_path):
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        else:
            df.to_csv(csv_path, index=False, encoding='utf-8-sig', mode='a', header=False)
        
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜æ‰¹æ¬¡æ•°æ®å¤±è´¥: {e}")
        return False

def update_log(stock_count, status='completed'):
    """æ›´æ–°æ—¥å¿—æ–‡ä»¶"""
    try:
        log_data = {
            "æ›´æ–°æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "è‚¡ç¥¨æ•°é‡": stock_count,
            "æ•°æ®æº": "akshare",
            "çŠ¶æ€": status,
            "å¯ç”¨æ•°æ®æº": {
                "akshare": AKSHARE_AVAILABLE
            },
            "æ–‡ä»¶è·¯å¾„": "cache/stockA_fundamentals_akshare.csv",
            "åŒ…å«å­—æ®µ": [
                "è‚¡ç¥¨ä»£ç ", "è‚¡ç¥¨åç§°", "è‚¡ç¥¨ä¸Šå¸‚æ—¥æœŸ", "è‚¡ç¥¨ä¸Šå¸‚åœ°ç‚¹", "è‚¡ç¥¨æ‰€å±è¡Œä¸š",
                # ç›ˆåˆ©èƒ½åŠ›
                "æ¯è‚¡æ”¶ç›Š", "æ¯è‚¡å‡€èµ„äº§", "å‡€èµ„äº§æ”¶ç›Šç‡", "æ€»èµ„äº§æ”¶ç›Šç‡", "æ¯›åˆ©ç‡", "å‡€åˆ©ç‡", "è¥ä¸šåˆ©æ¶¦ç‡",
                # ä¼°å€¼æŒ‡æ ‡
                "å¸‚ç›ˆç‡ï¼ˆé™ï¼‰", "å¸‚ç›ˆç‡ï¼ˆTTMï¼‰", "å¸‚å‡€ç‡", "å¸‚é”€ç‡", "è‚¡æ¯ç‡",
                # æˆé•¿æ€§
                "è¥ä¸šæ”¶å…¥å¢é•¿ç‡", "å‡€åˆ©æ¶¦å¢é•¿ç‡", "å‡€èµ„äº§å¢é•¿ç‡", "å‡€åˆ©æ¶¦å¢é€Ÿ",
                # å¿å€ºèƒ½åŠ›
                "èµ„äº§è´Ÿå€ºç‡", "æµåŠ¨æ¯”ç‡", "é€ŸåŠ¨æ¯”ç‡",
                # è¿è¥èƒ½åŠ›
                "æ€»èµ„äº§å‘¨è½¬ç‡", "å­˜è´§å‘¨è½¬ç‡", "åº”æ”¶è´¦æ¬¾å‘¨è½¬ç‡",
                # ç°é‡‘æµ
                "æ¯è‚¡ç»è¥ç°é‡‘æµ", "ç°é‡‘æµé‡æ¯”ç‡"
            ]
        }
        
        log_path = 'cache/fundamentals_akshare_update_log.json'
        # å¦‚æœæ—¥å¿—æ–‡ä»¶å­˜åœ¨ï¼Œæ·»åŠ å†å²è®°å½•
        if os.path.exists(log_path):
            try:
                with open(log_path, 'r', encoding='utf-8') as f:
                    existing_log = json.load(f)
                if "å†å²è®°å½•" in existing_log:
                    existing_log["å†å²è®°å½•"].insert(0, log_data)
                    # ä¿ç•™æœ€è¿‘100æ¡å†å²è®°å½•
                    if len(existing_log["å†å²è®°å½•"]) > 100:
                        existing_log["å†å²è®°å½•"] = existing_log["å†å²è®°å½•"][:100]
                else:
                    existing_log["å†å²è®°å½•"] = [log_data]
                log_data = existing_log
            except Exception as e:
                logger.warning(f"è¯»å–å†å²æ—¥å¿—å¤±è´¥: {e}")
        else:
            log_data = {"å½“å‰çŠ¶æ€": log_data, "å†å²è®°å½•": [log_data]}
        
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        
        return True
    except Exception as e:
        print(f"âŒ æ›´æ–°æ—¥å¿—å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°ï¼šè·å–Aè‚¡è‚¡ç¥¨å®Œæ•´åŸºæœ¬é¢æ•°æ®ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ """
    logger.info("ğŸš€ å¼€å§‹è·å–Aè‚¡è‚¡ç¥¨å®Œæ•´åŸºæœ¬é¢æ•°æ® (akshareæ¨¡å¼)...")
    logger.info("ğŸ“Š è´¢åŠ¡æŒ‡æ ‡åŒ…æ‹¬ï¼šç›ˆåˆ©èƒ½åŠ›ã€ä¼°å€¼æŒ‡æ ‡ã€æˆé•¿æ€§ã€å¿å€ºèƒ½åŠ›ã€è¿è¥èƒ½åŠ›å’Œç°é‡‘æµ")
    
    # æ˜¾ç¤ºå¯ç”¨æ•°æ®æºçŠ¶æ€
    logger.info("ğŸ“¡ æ•°æ®æºçŠ¶æ€:")
    logger.info(f"   akshare: {'âœ… å¯ç”¨' if AKSHARE_AVAILABLE else 'âŒ æœªå®‰è£…'}")
    
    if not AKSHARE_AVAILABLE:
        logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®æºï¼Œè¯·å®‰è£…akshare")
        return
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    stock_list = get_stock_list()
    if stock_list is None:
        return
    
    stock_codes = stock_list['code'].astype(str).str.zfill(6).tolist()
    total_stocks = len(stock_codes)
    
    # åŠ è½½è¿›åº¦
    progress = load_progress()
    start_index = progress["last_index"]
    completed_codes = set(progress["completed_codes"])
    
    logger.info(f"ğŸ“Š å…±{total_stocks}åªè‚¡ç¥¨ï¼Œä»ç¬¬{start_index+1}åªå¼€å§‹è·å–...")
    logger.info(f"âœ… å·²å®Œæˆ{len(completed_codes)}åªè‚¡ç¥¨")
    
    if start_index >= total_stocks:
        logger.info("ğŸ‰ æ‰€æœ‰è‚¡ç¥¨æ•°æ®å·²è·å–å®Œæˆï¼")
        update_log(len(completed_codes))
        return
    
    # è·å–å¾…å¤„ç†çš„è‚¡ç¥¨
    remaining_codes = [code for code in stock_codes[start_index:] if code not in completed_codes]
    
    if not remaining_codes:
        logger.info("ğŸ‰ æ²¡æœ‰éœ€è¦è·å–çš„è‚¡ç¥¨æ•°æ®")
        update_log(len(completed_codes))
        return
    
    # åˆå§‹åŒ–æˆ–è¿½åŠ æ¨¡å¼
    mode = 'w' if start_index == 0 else 'a'
    
    # åˆ†æ‰¹è·å–æ•°æ®
    batch_size = 20  # æ¯æ‰¹å¤„ç†20åªè‚¡ç¥¨ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
    success_count = len(completed_codes)
    
    try:
        for i in range(0, len(remaining_codes), batch_size):
            batch_codes = remaining_codes[i:i+batch_size]
            batch_fundamentals = []
            
            current_start = start_index + i + 1
            current_end = min(start_index + i + batch_size, total_stocks)
            logger.info(f"\nğŸ”„ å¤„ç†ç¬¬{current_start}-{current_end}åªè‚¡ç¥¨...")
            
            # æ£€æŸ¥å½“å‰æ‰¹æ¬¡çš„æ€»ä½“è®¿é—®é¢‘ç‡
            if i > 0:  # ä¸æ˜¯ç¬¬ä¸€æ‰¹
                # æ ¹æ®é…ç½®æ·»åŠ æ‰¹æ¬¡é—´å»¶è¿Ÿ
                batch_delay = random.uniform(AntiCrawlConfig.BATCH_MIN_DELAY, AntiCrawlConfig.BATCH_MAX_DELAY)
                logger.info(f"   â±ï¸  æ‰¹æ¬¡é—´å»¶è¿Ÿ: {batch_delay:.2f}ç§’")
                time.sleep(batch_delay)
            
            for j, code in enumerate(batch_codes):
                # è®¾ç½®éšæœºUser-Agent
                headers = {'User-Agent': random.choice(AntiCrawlConfig.USER_AGENTS)}
                
                fundamental = get_fundamentals_real_data(code)
                
                if fundamental['è‚¡ç¥¨åç§°'] and fundamental['è‚¡ç¥¨åç§°'] != '':
                    batch_fundamentals.append(fundamental)
                    success_count += 1
                    completed_codes.add(code)
                    
                    # æ¯åªè‚¡ç¥¨æ˜¾ç¤ºè¿›åº¦
                    logger.info(f"   âœ… {code} - {fundamental['è‚¡ç¥¨åç§°']} å·²è·å–")
                else:
                    logger.warning(f"   âš ï¸  {code} æ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡")
                
                # é—´éš”æ—¶é—´é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(AntiCrawlConfig.STOCK_MIN_INTERVAL)
            
            # ä¿å­˜æ‰¹æ¬¡æ•°æ®
            if batch_fundamentals:
                if save_batch_to_csv(batch_fundamentals, mode):
                    current_index = start_index + i + len(batch_codes)
                    save_progress(current_index, list(completed_codes))
                    logger.info(f"   ğŸ’¾ æ‰¹æ¬¡æ•°æ®å·²ä¿å­˜ ({len(batch_fundamentals)}æ¡è®°å½•)")
                    mode = 'a'  # åç»­æ‰¹æ¬¡ä½¿ç”¨è¿½åŠ æ¨¡å¼
                    # ä¿å­˜å½“å‰è¿›åº¦åˆ°æ—¥å¿—
                    update_log(success_count, status='in_progress')
            else:
                logger.warning(f"   âš ï¸  æœ¬æ‰¹æ¬¡æ— æœ‰æ•ˆæ•°æ®")
        
        # æ›´æ–°æœ€ç»ˆæ—¥å¿—
        update_log(success_count)
        
        logger.info(f"\nğŸ‰ æ•°æ®è·å–å®Œæˆï¼")
        logger.info(f"ğŸ“Š æˆåŠŸè·å– {success_count}/{total_stocks} åªè‚¡ç¥¨çš„çœŸå®æ•°æ®")
        logger.info(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ° cache/stockA_fundamentals_akshare.csv")
        logger.info(f"ğŸ“ æ›´æ–°æ—¥å¿—å·²ä¿å­˜åˆ° cache/fundamentals_akshare_update_log.json")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        csv_path = 'cache/stockA_fundamentals_akshare.csv'
        if os.path.exists(csv_path):
            df = pd.read_csv(csv_path)
            logger.info(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡ï¼š")
            logger.info(f"   ğŸ“Š æ€»è®°å½•æ•°: {len(df)}")
            
            # æ˜¾ç¤ºæœ‰æ•ˆæ•°æ®ç»Ÿè®¡
            valid_data = df[df['è‚¡ç¥¨åç§°'].notna() & (df['è‚¡ç¥¨åç§°'] != '')]
            logger.info(f"   âœ… æœ‰æ•ˆæ•°æ®: {len(valid_data)}")
            
            if len(valid_data) > 0:
                # è¡Œä¸šåˆ†å¸ƒ
                industry_counts = valid_data['è‚¡ç¥¨æ‰€å±è¡Œä¸š'].value_counts()
                if len(industry_counts) > 0:
                    logger.info(f"   ğŸ¢ ä¸»è¦è¡Œä¸š: {industry_counts.head(3).to_dict()}")
                
                # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®
                logger.info(f"\nğŸ“‹ æ•°æ®é¢„è§ˆ:")
                preview_cols = ['è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'è‚¡ç¥¨æ‰€å±è¡Œä¸š', 'æ¯è‚¡æ”¶ç›Š', 'å‡€èµ„äº§æ”¶ç›Šç‡', 'å¸‚ç›ˆç‡ï¼ˆé™ï¼‰', 'å¸‚å‡€ç‡']
                available_cols = [col for col in preview_cols if col in valid_data.columns]
                logger.info(valid_data[available_cols].head().to_string())

    except KeyboardInterrupt:
        logger.warning(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œè¿›åº¦å·²ä¿å­˜")
        logger.warning(f"   å·²å®Œæˆ {success_count} åªè‚¡ç¥¨")
        save_progress(start_index + len(remaining_codes[:i+batch_size]), list(completed_codes))
        update_log(success_count, status='interrupted')

    except Exception as e:
        logger.error(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        logger.error(f"   å·²å®Œæˆ {success_count} åªè‚¡ç¥¨")
        save_progress(start_index + len(remaining_codes[:i+batch_size]), list(completed_codes))
        update_log(success_count, status='error')

if __name__ == "__main__":
    main()